\section{Introduction}

Neural Networks (NNs) are often \emph{overparameterized}, in the sense that their representational capacity far exceeds what is necessary for fitting training data.
Surprisingly, training overparameterized NNs via (variants of) Gradient Descent (GD) tends to produce solutions that generalize well, despite existence of many solutions that do not.
This \emph{implicit generalization} phenomenon attracted considerable scientific interest, resulting in various theoretical explanations (see, e.g., \cite{woodworth2020kernel,yun2020unifying,ZhangBHRV17,li2020towards,ji2018gradient,lyu2019gradient}).

Recent studies have surfaced a new form of implicit bias which arises in Recurrent Neural Networks (RNNs) and their variants (e.g., Long Short-Term Memory \cite{lstm} and Gated Recurrent Units \cite{chung2014empirical}).
For such models, the length of sequences in training is often shorter than in testing, and it is not clear to what extent a learned solution will be able to \emph{extrapolate} beyond the sequence lengths seen in training.
In the overparameterized regime, where the representational capacity of the learned model exceeds what is necessary for fitting short sequences, there may exist solutions which generalize but do not extrapolate, meaning that their accuracy is high over short sequences but arbitrarily poor over long ones (see \cite{cohen2022extrapolation}). 
In practice however, when training RNNs using GD, accurate extrapolation is often observed. 
We refer to this phenomenon as the \emph{implicit extrapolation} of~GD. 

As opposed to the implicit generalization of GD, little is formally known about its implicit extrapolation. 
Existing theoretical analyses of the latter focus on linear RNNs~---~also known as \emph{Linear Dynamical Systems} (LDS)~---~and either treat infinitely wide models \citep{emami2021implicit}, or models of finite width that learn from a memoryless teacher \citep{cohen2022extrapolation}.
In these regimes, GD has been argued to exhibit an implicit bias towards short-term memory.
While such results are informative, their generality remains in question, particularly since infinitely wide NNs are known to substantially differ from their finite-width counterparts, and since a memoryless teacher essentially neglects the main characteristic of RNNs (memory).

In this paper, we theoretically investigate the implicit extrapolation of GD when applied to overparameterized finite-width linear RNNs learning from a teacher with memory. 
We consider models with symmetric transition matrices, in the case where a student (learned model) with state space dimension~$d$ is trained on sequences of length~$k$ generated by a teacher with state space dimension~\smash{$\hat{d}$}.
Our interest lies in the overparameterized regime, where $d$ is greater than both $k$ and~\smash{$\hat{d}$}, meaning that the student has state space dimension large enough to fully agree with the teacher on sequences of length~$k$, while potentially disagreeing with it on longer sequences. As a necessary assumption on initialization, we follow prior work and focus on a certain balancedness condition, which is known (see experiments in \cite{cohen2022extrapolation}, as well as our theoretical analysis) to capture near-zero initialization as commonly employed in practice.

Our main theoretical result states that GD originating from a balanced initialization leads the student to extrapolate, \emph{irrespective of how large its state space dimension is}. 
Key to the result is a surprising connection to a moment matching theorem from \cite{cohen2011use}, whose proof relies on ideas from compressed sensing \citep{Elad2010Sparse,Eldar2021CompressedSensing} and neighborly polytopes \citep{gale1963neighborly}. 
This connection may be of independent interest, and in particular may prove useful in deriving other results concerning implicit properties of GD.
We corroborate our theory with experiments, which demonstrate extrapolation via learning low dimensional state spaces in both the analyzed setting and ones involving non-linear RNNs.

The implicit extrapolation of GD is a new and exciting area of inquiry.
Our results suggest that short-term memory is not enough for explaining it as previously believed.
We hope the techniques developed in this paper will contribute to a further understanding of this phenomenon.