\section{Linear Recurrent Neural Networks \label{sec:setup}}

Our theoretical analysis applies to single-input single-output (SISO) linear RNNs with symmetric transition matrices.
Given a state space dimension $d \in \mathbb{N}$, this model is defined by the update rule:
\begin{equation}
    \label{eq:lin_rnn}
    \vs_{t+1}=\mA \vs_t+\mB x_t,
    \quad
    y_t=\mC \vs_t,
    \quad
    t = 0 , 1 , 2 , \ldots
    \text{\,,}
\end{equation}
where
$\mA \in\mathbb{R}^{d\times d}$, $\mB \in \mathbb{R}^{d\times 1}$ and $\mC \in \mathbb{R}^{1\times d}$ are configurable parameters, with the \emph{transition matrix}~$\mA$ satisfying $\mA = \mA^\top$;
$x_0 , x_1 , \ldots \in \mathbb{R}$ form an input sequence;
$y_0 , y_1 , \ldots \in \mathbb{R}$ form the corresponding output sequence; and 
$\vs_t\in\mathbb{R}^{d\times 1}$ represents the internal state at time~$t$, assumed to be equal to zero at the outset (i.e.~it is assumed that $\vs_0=\mathbf{0}$).
As with any linear time-invariant system \citep{porat1996course}, the input-output mapping realized by the RNN is determined by its impulse response.
\begin{definition}\label{def:impulse_response}
The \textbf{impulse response} of the RNN is the output sequence corresponding to the input sequence $( x_0 , x_1 , x_2 , \ldots ) = ( 1,  0 , 0 , \ldots )$.
Namely, it is the sequence $(\mC\mB,\mC\mA\mB,\mC\mA^2\mB,\dots)$.
\end{definition}
For brevity, we employ the shorthand $\Theta := (\mA,\mB,\mC)$.
The $d \times d$ symmetric transition matrix~$\mA$ is parameterized through a $d ( d + 1 ) /2$-dimensional vector holding its upper triangular elements, and with a slight overloading of notation, the symbol~$\mA$ is also used to refer to this parameterization.

We note that our theory readily extends to multiple-input multiple-output (MIMO) networks, and the focus on the SISO case is merely for simplicity of presentation.
Note also that the restriction to symmetric transition matrices is customary in both theory \citep{hazan2018spectral} and practice \citep{gupta2022diagonal}, and represents a generalization of the \emph{canonical modal form}, which under mild non-degeneracy conditions does not limit generality \citep{boyd2006ee263}.
%. \amirg{I removed the sentence about canonical forms. I'd consider writing here that symmetric matrices do not limit the expressivity of the LDS, but this is only true for complex values.}
%, and falls under the realm of \emph{canonical forms}~---~an important topic in the LDS literature dealing with the study of structured transition matrices \nadav{cite some general reference}. %, could be a textbook}.
%\amirg{not clear what "falls under the realm" means here. I suggest we drop this. We can instead say that we generalize the case of diagonal A which is often studied in DL theory.}
%An important topic in the LDS literature is that of \emph{canonical forms}, which in our context deals with imposing structural assumptions on the parameters of an RNN.
%We will consider a parameterization in which the transition matrix of the student and teacher ($\mA$ and~$\hat{\mA}$, respectively) are symmetric. Such assumptions are common in theoretical analyses of LDS. For example \cite{hardt2016gradient} analyze the convergence properties of LDS in \emph{controllable canonical form}, and \cite{hazan2018spectral} suggest algorithms for the identification of LDS with symmetric transition matrices, which is similar to the parameterization considered in this paper. On the empirical side, RNNs with structured transition matrices have shown outstanding performance in tasks involving extremely long sequences \cite{gu2020hippo,gu2021combining,gu2022efficiently}. Even more recently, \citet{gupta2022diagonal} have shown that comparable performance can be attained when the transition matrix of the RNN is diagonal.

Given a length~$k$ input sequence, $\mathbf{x}=(x_0,\dots,x_{k-1}) \in \mathbb{R}^k$, consider the output at the last time step, i.e.~$y:=y_k \in \mathbb{R}$, and denote it by~$RNN ( \vx )$.
Using this output as a label, we define an empirical loss induced by a training set $S=\left\lbrace \left(\vx^{(1)},y^{(1)}\right),\dots, \left(\vx^{(N)},y^{(N)}\right) \right\rbrace \subset \mathbb{R}^k \times \mathbb{R}$:
\begin{equation}
    \mathcal{L}_S(\mA,\mB,\mC)=\frac{1}{N}\sum_{i=1}^N \ell\left( RNN\left(\vx^{(i)}\right), y^{(i)}\right),
\end{equation}
where $\ell(y,\hat{y})=(y-\hat{y})^2$ is the square loss. 
By the update rule of the RNN (\Eqref{eq:lin_rnn}), we have:
\begin{equation}\label{eq:empirical_loss}
    \mathcal{L}_S(\mA,\mB,\mC) = \frac{1}{N}\sum_{i=1}^N \left( \sum_{j=0}^{k-1}\mC\mA^{k-1-j}\mB x_j^{(i)} - y^{(i)} \right)^2.
\end{equation}
Suppose that ground truth labels are generated by an RNN as defined in \Eqref{eq:lin_rnn}, and denote the state space dimension and parameters of this \emph{teacher} network by \smash{$\hat{d}$} and \smash{$\hat{\Theta} = (\hat{\mA},\hat{\mB},\hat{\mC})$} respectively.
We employ the common assumption (e.g., see \cite{hardt2016gradient}) by which input sequences are drawn from a whitened distribution, i.e.~a distribution where $\E\left[ x_j x_{j'} \right]$ equals~$1$ if $j = j'$ and~$0$ otherwise.
The population loss over length~$k$ sequences can then be written as (see Lemma~\ref{lemma:expected_loss}):
\begin{equation}\label{eq:population_loss}
    \mathcal{L}(\mA,\mB,\mC)=\sum_{j=0}^{k-1} \left(\mC\mA^j \mB-\hat{\mC}\hat{\mA}^j\hat{\mB}\right)^2
    \text{\,.}
\end{equation}
\Eqref{eq:population_loss} implies that a solution $\Theta = ( \mA , \mB , \mC )$ achieves zero population loss over length~$k$ sequences if and only if \smash{$\mC \mA^j \mB=\hat{\mC} \hat{\mA}^j \hat{\mB}$} for $j=0,\dots,k-1$.
To what extent does such a solution imply that the \emph{student} (i.e., the learned RNN) extrapolates to longer sequences?
This depends on how close $\mC \mA^j \mB$ is to \smash{$\hat{\mC} \hat{\mA}^j \hat{\mB}$} for $j \geq k$.
\begin{definition}\label{def:extrapolation}
For $\epsilon \geq 0$ and $q \in \mathbb{N}$, we say that the student \textbf{$\epsilon$-extrapolates with horizon $q$} with respect to (w.r.t) the teacher if: 
\begin{equation}
    |\mC\mA^j \mB-\hat{\mC}\hat{\mA}^j\hat{\mB}| \leq \epsilon,
    \quad
    \forall j \in \{ 0 , 1 , \ldots , q - 1 \}
    \text{\,.}
\end{equation}
If the above holds for all $q \in \mathbb{N}$ then the student is said to \textbf{$\epsilon$-extrapolate} w.r.t the teacher, and if it holds for all $q \in \mathbb{N}$ with $\epsilon = 0$ then the student is simply said to \textbf{extrapolate} w.r.t the teacher.
\end{definition}
Per Definition~\ref{def:extrapolation}, $\epsilon$-extrapolation with horizon~$q$ is equivalent to the first $q$~elements of the student's impulse response being $\epsilon$-close to those of the teacher's, whereas extrapolation means that the student's impulse response fully coincides with the teacher's.
The latter condition implies that the student realizes the same input-output mapping as the teacher, for any sequence length (this corresponds to the notion of system identification; see \Secref{sec:related_work}).
When the student is \emph{overparameterized}, in the sense that $d$ is greater than $k$ and~\smash{$\hat{d}$}, it may perfectly generalize, i.e.~lead the population loss over length~$k$ sequences (\Eqref{eq:population_loss}) to equal zero, and yet fail to extrapolate as stated in the following proposition.
\begin{proposition}\label{prop:symmetric_lds_expressivity}
Assume $d > k$, and let $\epsilon \geq 0$ and $q \in \{ k + 1 , k + 2 , \ldots\}$.
Then, for any teacher parameters \smash{$\hat{\Theta}$}, there exist student parameters $\Theta$ with which the population loss in \Eqref{eq:population_loss} equals zero, and yet the student does \emph{not} $\epsilon$-extrapolate with horizon $q$.
\end{proposition}
\begin{sproof}[Proof sketch (for complete proof see \Appref{sec:apdx:perfect_generalization_failed_extrapolation}).]
The result follows from the fact that the first~$d$ elements of the student's impulse response can be assigned freely via a proper choice of~$\Theta$.
\end{sproof}
% MOVE THE PROOF TO APPENDIX
% \begin{proof}
%     Consider a student, $\Theta$, such that $\mA$ is symmetric (and therefore has an orthogonal eigendecomposition). Denote $\mA=\mU\Lambda \mU^\top$. The impulse response at time step $i$ can be expressed as $\mC\mA^i\mB=\mC\mU\Lambda^i \mU^\top \mB$. The latter can be written compactly in matrix form as $\mV\vg$ where $\mV$ is the Vandermonde matrix with $diag(\Lambda)$ as its values,
%     \begin{equation*}
%         V=\begin{pmatrix}
%             1 & 1 & \dots & 1\\
%             \lambda_1 & \lambda_2 & \dots & \lambda_d\\
%             \lambda_1^2 & \lambda_2^2 & \dots & \lambda_d^2\\
%             \vdots & \vdots & & \vdots \\
%             \lambda_1^{d-1} & \lambda_2^{d-1} & \dots & \lambda_d^{d-1}\\
%         \end{pmatrix}
%     \end{equation*}
%     and $vg$ is defined as $\vg\equiv (\mC\mU)^\top\circ \mU^\top \mB$.\footnote{Here $\circ$ denotes the Hadamard (elementwise) product.} A known result on square Vandermonde matrices is that they are invertible if and only if $\lambda_i\neq \lambda_j,\; \forall i\neq j$. Given a fixed set of distinct values $(\lambda_1,\dots,\lambda_d)$ and an arbitrary impulse response $\vr\in\mathbb{R}^d$, in order for the student to generate the impulse response $\vr$ (i.e. $\mV\vg=\vr$), one can set the coefficient vector, $\vg=\mV^{-1}\vr$ and end up with a symmetric student with $\vr$ as its impulse response of length $d$.
% \end{proof}
We are interested in the extent to which student parameters learned by GD extrapolate in the overparameterized regime.
Proposition~\ref{prop:symmetric_lds_expressivity} implies that, regardless of how many (length~$k$) sequences are used in training, if GD leads to any form of extrapolation, it must be a result of some implicit bias induced by the algorithm.
Note that in our setting, extrapolation cannot be explained via classic tools from statistical learning theory, as evaluation over sequences longer than those seen in training violates the standard assumption of train and test data originating from the same distribution.

To decouple the question of extrapolation from that of generalization, we consider the case where the training set~$S$ is large, or more formally, where the empirical loss~$\mathcal{L}_S ( \cdot )$ (\Eqref{eq:empirical_loss}) is well represented by the population loss~$\mathcal{L} ( \cdot )$ (\Eqref{eq:population_loss}). 
We model GD with small step size via Gradient Flow (GF), as customary in the theory of NNs~---~see \citet{saxe2013exact, gunasekar2017implicit, arora2018optimization, arora2019implicit, lyu2019gradient, li2020towards, azulay2021implicit} for examples and~\cite{elkabetz2021continuous} for theoretical justification.
That is, we analyze the following dynamics:
\begin{equation}
    \dot{\alpha} ( \tau ) := \frac{d}{d \tau} \alpha ( \tau ) = - \frac{\partial}{\partial \alpha} \mathcal{L} \big( \mA ( \tau ) , \mB ( \tau ) , \mC ( \tau ) \big)
    ~ , ~
    \tau \geq 0
    \text{\,,}
\end{equation}
where $\alpha\in\lbrace \mA,\mB,\mC \rbrace$. 
If no assumption on initialization is made, no form of extrapolation can be established (indeed, the initial point may be a global minimizer of~$\mathcal{L} ( \cdot )$ that fails to extrapolate, and GF will stay there).
Following prior work (see \cite{cohen2022extrapolation}), we assume that initialization adheres to the following balancedness condition:
\begin{definition}\label{def:balanced}
An RNN with parameters $\Theta = (\mA,\mB,\mC)$ is said to be \textbf{balanced} if $\mB=\mC^\top$.
\end{definition}
It was shown empirically in \cite{cohen2022extrapolation} that the balancedness condition captures near-zero initialization as commonly employed in practice.
We support this finding theoretically in Section~\ref{sec:relaxed_balanced_assumption}.
Aside from the initialization of the student, we will also assume that the teacher adheres to the balancedness condition.


% \st{We will consider a form known as \textit{diagonal} (or \textit{modal}), which corresponds to the case where the transition matrices of the student and teacher ($A$ and}~\smash{$\hat{A}$},\st{ respectively) are diagonal.
% This falls in line with the recent theoretical focus on diagonal linear (feed-forward) NNs~---~see (cite recent works analyzing diagonal linear NNs.)}}.
%
%As a final restriction in the analyzed setup, we will assume that the transition matrix of the learned RNN, i.e.~its parameter matrix~$A$, is diagonal.
%In the LDS literature, diagonal transition matrices correspond to a canonical form known as \textit{Modal}.
%It is a common practice in the theory of NNs to analyze diagonal parameter matrices and canonical forms for LDS (see for example \citep{hardt2016gradient,woodworth2020kernel,yun2020unifying,gupta2022diagonal}).
%
%Finally, to simplify the analysis, we focus on the case where the RNN transition matrix $A$ is diagonal. It is important to note the diagonal assumption does not reduce expressivity as every LDS admits a canonical \textit{Modal} form in which the transition matrix is diagonal.\amirg{need to see about complex eigenvalues. Raja: I think we should add the word symmetric for this to hold true. Also why not add a reference to \citet{gupta2022diagonal} here to support the diagonal claim} We remark that related diagonal assumptions were taken in analysis of implicit bias of GD in linear and non-linear networks \citep{woodworth2020kernel,yun2020unifying}. In the context of LDS, \citet{hardt2016gradient} have used the Controllable Canonical form of LDS as the subject of their analysis. These analyses have provided considerable insight about GD, and we believe this is also the case here. 