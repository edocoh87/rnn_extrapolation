\section{Conclusion}\label{sec:conclusions}

This paper studies the question of extrapolation in RNNs, and more specifically, of whether a student RNN trained on data generated by a teacher RNN can capture the behavior of the teacher over sequences longer than those seen in training. 
We focus on overparameterized students that can perfectly fit training sequences while producing a wide range of behaviors over longer sequences. 
Such a student will fail to extrapolate, unless the teacher possesses a certain structure, and the learning algorithm is biased towards solutions adhering to that structure.
We show~---~theoretically for linear RNNs and empirically for both linear and non-linear RNNs~---~that such implicit extrapolation takes place when the teacher has a low dimensional state space and the learning algorithm is GD.%\amirg{Need to add that the teacher is balanced.}

Existing studies of implicit extrapolation in (linear) RNNs \citep{emami2021implicit,cohen2022extrapolation} suggest that GD is biased towards solutions with short-term memory.
While low dimensional state space and short-term memory may coincide in some cases, in general they do not, and a solution with low dimensional state space may entail long-term memory.
Our theory and experiments show that in settings where low dimensional state space and short-term memory contradict each other, the implicit extrapolation chooses the former over the latter.%\amirg{do we show this explicitly? I don't think so. This sounds too strong.} \ec{see comment in overleaf on this.}

We note that linear RNNs fundamentally differ from the commonly studied model of linear (feed-forward) NNs (see, e.g., \cite{arora2018optimization,ji2018gradient,arora2019convergence,arora2019implicit,razin2020implicit}). 
One of the key differences is that a linear RNN entails different powers of a parameter (transition) matrix, leading to a loss function which roughly corresponds to a sum of losses for multiple linear NNs having different architectures and shared weights.
This makes the analysis of linear RNNs technically challenging, warranting introduction of new theoretical approaches such as the moment matching view employed in \Secref{sec:exact_extrapolation}. 

An important direction for future work is extending our theory to non-linear RNNs. 
We believe it is possible, in the same way that theories for linear (feed-forward) NNs were extended to account for non-linear NNs (see, e.g., \citet{razin2021implicit,razin2022implicit, lyu2019gradient}).
An additional direction to explore is the applicability of our results to the recently introduced S4 model \citep{gu2022efficiently}.